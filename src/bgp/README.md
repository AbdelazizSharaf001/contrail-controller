This directory contains the [BGP](http://www.rfc-editor.org/rfc/rfc4271.txt) engine used by the Contrail control-node.

It supports standard [BGP L3VPN](http://tools.ietf.org/html/rfc4364) routing functionality as well as current work-in-progress
specifications designed to extend virtual network support to [end-systems](http://tools.ietf.org/html/draft-ietf-l3vpn-end-system-01)

The BGP engine is designed from the ground up to support concurrency and a [TDD](http://en.wikipedia.org/wiki/Test-driven_development) development methodology.

From a concurrency standpoint, the engine operates as a pipeline with 3 independent stages:
- input processing;
- database manipulation (including update queuing);
- update message encoding;

Each stage can then in-turn exhibit data parallelism:
- Input message validation and decoding is independent ammong different BGP/XMPP peers
- The database is partitioned in shards by hashing the key (NLRI prefix)
- Output message encoding is performed in parallel for different scheduling-groups of BGP/XMPP peers.

Mutex style locking is required for code that manipulates data-structures accessed by multiple stages in the pipeline but not for data-structures that are local to a single stage.
To goal is to enable incremental feature development with minimal concern for concurrency. Message decoding and encoding is largely autogenerated via a DSL implemented
as C++ templates (bgp_proto.cc). Additions to the database manipulation code in order to support extra functionality do not require locks
since the DB partitions represent independent data-sets.

The code is organized such that each class is independetly testable via a UT in the test directory. Tests are automatically executed when running
"SCons" on the "src/bgp" directory.

The core functionality of the engine is the update enqueuing and dequeing code. Changes in the database entries
cause the code to calculate the desired attributes for the DB entry at the receiver; this functionality happens on
a per DB-partition basis and for each group of peers. Peers may be placed in different groups because they have different policy
on how to calculate desired DB entry attributes (policy) or CPU affinity.

When an DB entry changes, the address family specific code is called from BgpExport::Export() to generate the desired outbound attributes.
The same method calculates the delta between the previously advertised state for that key and the new desired state and enqueues and update if necessary.
The code is desined to be able to suppress duplicate messages or transitent changes. Access to the update queues is mediated
by the RibUpdateMonitor class. This monitor deals with the contention between N producers (a DB partition per virtual CPU core) and a single
consumer (the scheduling-group task). The scheduling-group task dequeues update entries for the queue and formats the required message according to the
peer's encapsulation (BGP/XMPP).

Since different peers within a scheduling-group may process updates at different rates (due to different CPU, network latency or just entropy)
these peers may be at different postitions in the processing queue. The update queue is composed of both updates and markers which record the position at which one or more peers within
the group are at.
